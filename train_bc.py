# train_bc.py 

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import os
from tqdm import tqdm
import argparse # New import for command-line arguments

# --- 1. The Neural Network ---
# A simple Multi-Layer Perceptron (MLP).
class BCNet(nn.Module):
    def __init__(self, obs_size=134, action_size=2):
        super(BCNet, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(obs_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_size),
            nn.Tanh() # Actions are often scaled between -1 and 1. Tanh is perfect for this.
        )

    def forward(self, x):
        return self.network(x)

# --- 2. The Dataset Loader ---
# This class will load the .npz files generated by log_distill.py
class DistillDataset(Dataset):
    def __init__(self, data_dir):
        self.data_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.npz')]

        all_obs = []
        all_actions = []
        print(f"Loading data from {len(self.data_files)} files...")
        for file_path in tqdm(self.data_files):
            with np.load(file_path) as data:
                all_obs.append(data['obs'])
                all_actions.append(data['action'])

        self.observations = np.concatenate(all_obs, axis=0)
        self.actions = np.concatenate(all_actions, axis=0)
        print(f"Loaded {len(self.observations)} total steps.")

    def __len__(self):
        return len(self.observations)

    def __getitem__(self, idx):
        obs = torch.from_numpy(self.observations[idx]).float()
        act = torch.from_numpy(self.actions[idx]).float()
        return obs, act

# --- 3. The Main Training Loop ---
if __name__ == "__main__":
    # --- New: Setup command-line argument parsing ---
    parser = argparse.ArgumentParser(description="Train a Behavior Cloning policy for the robot game.")
    parser.add_argument("--output-file", "-o", type=str, default="bc_policy.pth",
                        help="Path to save the trained model file.")
    parser.add_argument("--input-file", "-i", type=str, default=None,
                        help="Optional path to a pre-trained model to continue training from.")
    args = parser.parse_args()

    # Hyperparameters
    DATA_DIR = "logs_npz" # Assuming data is in this directory
    EPOCHS = 100
    BATCH_SIZE = 64
    LEARNING_RATE = 1e-4
    VAL_SPLIT = 0.1 # Use 10% of data for validation

    # Set device (use GPU if available)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load dataset
    dataset = DistillDataset(DATA_DIR)

    # Split into training and validation sets
    val_size = int(len(dataset) * VAL_SPLIT)
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

    # Initialize model
    model = BCNet().to(device)

    # --- New: Load weights from input file if provided ---
    if args.input_file:
        if os.path.exists(args.input_file):
            print(f"Loading model weights from '{args.input_file}' to continue training.")
            model.load_state_dict(torch.load(args.input_file, map_location=device))
        else:
            print(f"Warning: Input file '{args.input_file}' not found. Starting with a new model.")

    # Initialize loss function and optimizer
    loss_fn = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # Training loop
    for epoch in range(EPOCHS):
        model.train()
        train_loss = 0.0
        for obs_batch, act_batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Training]"):
            obs_batch, act_batch = obs_batch.to(device), act_batch.to(device)
            pred_actions = model(obs_batch)
            loss = loss_fn(pred_actions, act_batch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for obs_batch, act_batch in val_loader:
                obs_batch, act_batch = obs_batch.to(device), act_batch.to(device)
                pred_actions = model(obs_batch)
                loss = loss_fn(pred_actions, act_batch)
                val_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        print(f"Epoch {epoch+1}/{EPOCHS} -> Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}")

    # --- Modified: Save the trained model to the specified output file ---
    torch.save(model.state_dict(), args.output_file)
    print(f"\nTraining complete! Model saved to '{args.output_file}'")
