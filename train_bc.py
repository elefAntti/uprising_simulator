# This is the GEMINI generated PoC training script
# train_bc.py

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
import numpy as np
import os
from tqdm import tqdm

# --- 1. The Neural Network ---
# A simple Multi-Layer Perceptron (MLP).
class BCNet(nn.Module):
    def __init__(self, obs_size=134, action_size=2):
        super(BCNet, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(obs_size, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_size),
            nn.Tanh() # Actions are often scaled between -1 and 1. Tanh is perfect for this.
        )

    def forward(self, x):
        return self.network(x)

# --- 2. The Dataset Loader ---
# This class will load the .npz files generated by log_distill.py
class DistillDataset(Dataset):
    def __init__(self, data_dir):
        self.data_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.npz')]
        
        # Load all data into memory for this proof-of-concept
        # For larger datasets, you might load files on-demand
        all_obs = []
        all_actions = []
        print(f"Loading data from {len(self.data_files)} files...")
        for file_path in tqdm(self.data_files):
            with np.load(file_path) as data:
                all_obs.append(data['obs'])
                all_actions.append(data['action'])
        
        self.observations = np.concatenate(all_obs, axis=0)
        self.actions = np.concatenate(all_actions, axis=0)
        print(f"Loaded {len(self.observations)} total steps.")

    def __len__(self):
        return len(self.observations)

    def __getitem__(self, idx):
        obs = torch.from_numpy(self.observations[idx]).float()
        act = torch.from_numpy(self.actions[idx]).float()
        return obs, act

# --- 3. The Main Training Loop ---
if __name__ == "__main__":
    # Hyperparameters
    DATA_DIR = "logs_npz"
    EPOCHS = 200
    BATCH_SIZE = 64
    LEARNING_RATE = 1e-4
    VAL_SPLIT = 0.1 # Use 10% of data for validation

    # Set device (use GPU if available)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Load dataset
    dataset = DistillDataset(DATA_DIR)
    
    # Split into training and validation sets
    val_size = int(len(dataset) * VAL_SPLIT)
    train_size = len(dataset) - val_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

    # Initialize model, loss function, and optimizer
    model = BCNet().to(device)
    loss_fn = nn.MSELoss() # Mean Squared Error is great for regression problems like this
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # Training loop
    for epoch in range(EPOCHS):
        # Training phase
        model.train()
        train_loss = 0.0
        for obs_batch, act_batch in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS} [Training]"):
            obs_batch = obs_batch.to(device)
            act_batch = act_batch.to(device)

            # Forward pass
            pred_actions = model(obs_batch)
            loss = loss_fn(pred_actions, act_batch)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()

        # Validation phase
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for obs_batch, act_batch in val_loader:
                obs_batch = obs_batch.to(device)
                act_batch = act_batch.to(device)
                pred_actions = model(obs_batch)
                loss = loss_fn(pred_actions, act_batch)
                val_loss += loss.item()

        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        
        print(f"Epoch {epoch+1}/{EPOCHS} -> Train Loss: {avg_train_loss:.6f}, Val Loss: {avg_val_loss:.6f}")

    # Save the trained model
    MODEL_PATH = "bc_policy.pth"
    torch.save(model.state_dict(), MODEL_PATH)
    print(f"\nTraining complete! Model saved to {MODEL_PATH}")
